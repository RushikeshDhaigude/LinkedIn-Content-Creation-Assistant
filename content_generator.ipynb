{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67674636",
   "metadata": {},
   "source": [
    "### Linkedin Content Generator\n",
    "\n",
    "\n",
    "Title: LinkedIn Content Co-Pilot — research-grounded, on-brand post generator with human-in-the-loop approvals, analytics feedback, and safe auto-scheduling.\n",
    "\n",
    "One-liner: Give it a topic → it researches, drafts multiple post variants (hook/body/CTA/hashtags), cites sources, enforces LinkedIn limits, checks tone & safety, and schedules via Buffer (or exports for manual post). Learns from your past high-performing posts to mimic your voice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe307df4",
   "metadata": {},
   "source": [
    "#### High-level architecture (local → cloud)\n",
    "\n",
    "##### Local dev (fast to demo):\n",
    "\n",
    "1. Streamlit UI for topic input + preview/approve + export.\n",
    "\n",
    "2. LangGraph (orchestrates multi-step agent flow).\n",
    "\n",
    "3. LLMs (Gemini / OpenAI) for writing & editing.\n",
    "\n",
    "4. Retrieval tools:\n",
    "\n",
    "   - Web search + page scraping (for facts) → embedded + stored to a small local vector DB (Chroma).\n",
    "\n",
    "   - Personal “voice profile” built from your past top posts (CSV/Markdown drop-in).\n",
    "\n",
    "5. Guardrails: moderation (LLM + rules), claim/URL check, plagiarism/dup-content check (embedding similarity to sources and past posts).\n",
    "\n",
    "6. Outputs: Markdown/JSON/CVS export, optional Buffer API schedule, or Google Sheet handoff.\n",
    "\n",
    "##### Production (optional):\n",
    "\n",
    "1. AWS: Step Functions (orchestrate), Lambda (workers), S3 (artifacts), DynamoDB (posts + metrics), Secrets Manager (keys), EventBridge (cron), CloudWatch (logs).\n",
    "\n",
    "2. Alternative: run daily on GitHub Actions + Render/Fly.io for the Streamlit UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23f3dee",
   "metadata": {},
   "source": [
    "#### The LangGraph workflow (agents & tools)\n",
    "\n",
    "##### 1. Brief Planner\n",
    "\n",
    "- Input: topic + desired audience + length + tone.\n",
    "\n",
    "- Output: content brief (angle, 3 key points, working title, target outcomes).\n",
    "\n",
    "##### 2. Researcher\n",
    "\n",
    "- Tools: web search + HTML fetcher → chunk → embeddings → top-k evidence.\n",
    "\n",
    "- Output: structured evidence with URLs + key facts + quotes.\n",
    "\n",
    "##### 3. Factuality Checker\n",
    "\n",
    "- Ensures each claim in the brief maps to evidence (or is clearly labeled as opinion).\n",
    "\n",
    "- Flags weak/unsupported claims to revise.\n",
    "\n",
    "##### 4. Writer\n",
    "\n",
    "Drafts 2–3 variants:\n",
    "\n",
    "  - Variant A: educational mini-essay (≤ 1,300 chars).\n",
    "\n",
    "  - Variant B: hook-heavy listicle.\n",
    "\n",
    "  - Variant C: story/anecdote format (optional).\n",
    "\n",
    "All include: hook, body, CTA, 5–8 hashtags, 2–3 source links.\n",
    "\n",
    "##### 5. Voice Styler\n",
    "\n",
    "- Uses your “voice profile” (extracted from past posts: cadence, formality, emoji usage, average sentence length, buzzword tolerance).\n",
    "\n",
    "- Rewrites to match.\n",
    "\n",
    "##### 6. Compliance & Safety Gate\n",
    "\n",
    "- Checks: length limits, profanity/toxicity, sensitive claims without sources, company NDAs, hallucination risk (no source → softened phrasing).\n",
    "\n",
    "- Suggests fixes automatically.\n",
    "\n",
    "##### 7. Editor\n",
    "\n",
    "- Tightens verbs, removes hedging, ensures scannability (short paragraphs, bullets), adds 1 CTA, no more than 1 question, removes filler.\n",
    "\n",
    "##### 8. Scorer\n",
    "\n",
    "- Heuristics + small LLM rubric to score: hook strength, specificity, actionability, credibility signals, and skim-ability.\n",
    "\n",
    "##### 9. A/B Selector + Scheduler\n",
    "\n",
    "- Picks top 2 variants, staggers publish times (e.g., Tue/Thu 9:15 AM IST) via Buffer API (or exports to CSV/Markdown).\n",
    "\n",
    "- UTM params for link CTAs.\n",
    "\n",
    "##### 10. Analytics Feedback (post-hoc loop)\n",
    "\n",
    "- Ingest likes/comments/impressions (manual CSV upload or Buffer/Shield export).\n",
    "\n",
    "- Learns what features correlate with higher engagement → updates prompt knobs (e.g., “hooks with numbers outperform”).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c0d5a9",
   "metadata": {},
   "source": [
    "#### Data & storage\n",
    "\n",
    "/data/voice/: your previous posts (CSV/MD).\n",
    "\n",
    "/data/cache/: scraped pages + chunked embeddings (Chroma).\n",
    "\n",
    "/artifacts/: generated posts + evidence packs (JSON + MD)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820a537b",
   "metadata": {},
   "source": [
    "#### Model strategy (practical + impressive)\n",
    "\n",
    "1. Generation: Gemini 1.5 Flash (fast) or OpenAI GPT-4o-mini; configurable.\n",
    "\n",
    "2. Editing/Refinement: same model with strict, short system prompts.\n",
    "\n",
    "3. Retrieval: embeddings (bge/multilingual or OpenAI text-embedding-3-small) in Chroma.\n",
    "\n",
    "4. Evaluation: small LLM judging + regex/rules for length & links.\n",
    "\n",
    "5. Prompt Engineering:\n",
    "\n",
    "   - ReAct style for research → claim-evidence table.\n",
    "\n",
    "   - “Voice tokens” distilled from your history (e.g., avg sentence length, emoji rate, preferred sign-offs).\n",
    "\n",
    "   - Style constraints for LinkedIn (line breaks, no hashtags mid-body, CTA at end)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168cb037",
   "metadata": {},
   "source": [
    "#### Guardrails & compliance\n",
    "\n",
    "1. LinkedIn API: Prefer official/compliant posting via Buffer/Hootsuite APIs. Avoid browser automation that violates ToS.\n",
    "\n",
    "2. Attribution: Include 2–3 credible sources with explicit URLs.\n",
    "\n",
    "3. No unverified stats: If model can’t find a source, soften claim (“In my experience…”).\n",
    "\n",
    "4. Moderation: run a toxicity/offensive check + a “claims without sources” check.\n",
    "\n",
    "5. Plagiarism/dup: cosine similarity against sources and your own corpus; flag if too high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68df5a5f",
   "metadata": {},
   "source": [
    "#### Analytics & A/B testing\n",
    "\n",
    "1. Generate 2 variants per topic; schedule on different days/times.\n",
    "\n",
    "2. Track metrics (ingest CSV exports).\n",
    "\n",
    "3. Simple uplift analysis: per-feature regression or SHAP on handcrafted features (hook length, #numbers, #bullets, questions yes/no).\n",
    "\n",
    "4. Close the loop: persist the best feature ranges into a small config.yaml used by prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b0c99",
   "metadata": {},
   "source": [
    "#### Deliverables\n",
    "\n",
    "1. Streamlit app (demo video + live): topic → research preview → choose variants → schedule/export → see past performance.\n",
    "\n",
    "2. CLI (copilot generate --topic \"X\" --schedule tomorrow 9:15).\n",
    "\n",
    "3. LangGraph diagram auto-rendered as PNG.\n",
    "\n",
    "4. MLflow runs for prompt variants (store prompts, scores, token cost).\n",
    "\n",
    "5. Dockerfile + docker compose up.\n",
    "\n",
    "6. Optional AWS deploy: Terraform/IaC snippets for core resources.\n",
    "\n",
    "7. Unit tests for tools, and prompt regression tests with golden outputs.\n",
    "\n",
    "8. README with security/keys, ToS notes, and a “what I’d improve next” section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7869ec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6a1ab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_rag.py\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from pypdf import PdfReader\n",
    "import re\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict, Annotated\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "#import google.generativeai as genai\n",
    "from langchain_core.tools import tool\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4bc664",
   "metadata": {},
   "source": [
    "#### Config Loader\n",
    "app/config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb38f221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3ccde3",
   "metadata": {},
   "source": [
    "#### Tools Layer\n",
    "Search Tool\n",
    "\n",
    "app/tools/search_tool.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bfd230c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "def search_web_ddg(query: str) -> list[dict]:\n",
    "    \"\"\"Search the web using DuckDuckGo and return a list of results.\"\"\"\n",
    "    search_tool = DuckDuckGoSearchRun()\n",
    "    results = search_tool.invoke(query)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eaf9cce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amber/Documents/Learnings_2024/Linkdein_content_generator/linkdein_genai_env/lib/python3.10/site-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No good DuckDuckGo Search Result was found'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_web_ddg(\"AI engineer vs data scientist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba5f03",
   "metadata": {},
   "source": [
    "#### Scraping Tool\n",
    "app/tools/scrape_tool.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bec5e429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trafilatura\n",
    "\n",
    "def scrape_url(url):\n",
    "    \"\"\"Download and extract main text from a web page.\"\"\"\n",
    "    downloaded = trafilatura.fetch_url(url)\n",
    "    if downloaded:\n",
    "        return trafilatura.extract(downloaded)\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d53d734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You need to consider how to orchestrate the multi-step workflow, keep track of the agents’ states, implement necessary guardrails, and monitor decision processes as they happen.\\nFortunately, LangGraph addresses exactly those pain points for you.\\nRecently, Google just demonstrated this perfectly by open-sourcing a full-stack implementation of a Deep Research Agent built with LangGraph and Gemini (with Apache-2.0 license).\\nThis isn’t a toy implementation: the agent can not only search, but also dynamically evaluate the results to decide if more information is needed by doing further searches. This iterative workflow is exactly the kind of thing where LangGraph really shines.\\nSo, if you want to learn how LangGraph works in practice, what better place to start than a real, working agent like this?\\nHere’s our game plan for this tutorial post: We’ll adopt a “problem-driven” learning approach. Instead of starting with lengthy, abstract concepts, we’ll jump right into the code and examine Google’s implementation. After that, we’ll connect each piece back to the core concepts of LangGraph.\\nBy the end, you’ll not only have a working research agent but also enough LangGraph knowledge to build whatever comes next.\\nAll the code we’ll be discussing in this post comes from the official Google Gemini repository, which you can find here. Our focus will be on the backend logic (backend/src/agent/ directory) where the research agent is defined.\\nHere is the visual roadmap for this post:\\n1. The Big Picture — Modeling the Workflow with Graphs, Nodes, and Edges\\n🎯 The problem\\nIn this case study, we’ll build something exciting: an LLM-based research-agumented agent, the minimal replication of the Deep Research features you’ve already seen in ChatGPT, Gemini, Claude, or Perplexity. That’s what we’re aiming for here.\\nSpecifically, our agent will work like this:\\nIt takes in a user query, autonomously searches the web, examines the search results it obtains, and then decide if enough information has been found. If that’s the case, it proceeds with creating a well-crafted mini-report with proper citations; Otherwise, it circles back to dig deeper with more searches.\\nFirst things first, let’s sketch out a high-level flowchart so that we are clear what we’re building here:\\n💡LangGraph’s solution\\nNow, how should we model this workflow in LangGraph? Well, as the name suggests, LangGraph uses graph representations. Ok, but why use graphs?\\nThe short answer is this: graphs are great for modeling complex, stateful flows, just like the application we aim to build here. When you have branching decisions, loops that need to circle back, and all the other messy realities that real-world agentic workflow would throw at you, graphs give you one of the most natural ways to represent them all.\\nTechnically, a graph is composed of nodes and edges. In LangGraph’s world, nodes are individual processing steps in the workflow, and edges define transitions between steps, that is, defining how control and state flow through the system.\\n</> Let’s see some code!\\nIn LangGraph, the translation from flowchart to code is straightforward. Let’s look at agent/graph.py\\nfrom the Google repository to see how this is done.\\nThe first step is to create the graph itself:\\nfrom langgraph.graph import StateGraph\\nfrom agent.state import (\\nOverallState,\\nQueryGenerationState,\\nReflectionState,\\nWebSearchState,\\n)\\nfrom agent.configuration import Configuration\\n# Create our Agent Graph\\nbuilder = StateGraph(OverallState, config_schema=Configuration)\\nHere, StateGraph\\nis LangGraph’s builder class for a state-aware graph. It accepts anOverallState\\nclass that defines what information can move between nodes (this is the agent memory part we will discuss in the next section), and a Configuration\\nclass that defines runtime-tunable parameters, such as which LLM to call at individual steps, the number of initial queries to generate, etc. More details on this will follow in the next sections.\\nOnce we have the graph container, we can add nodes to it:\\n# Define the nodes we will cycle between\\nbuilder.add_node(\"generate_query\", generate_query)\\nbuilder.add_node(\"web_research\", web_research)\\nbuilder.add_node(\"reflection\", reflection)\\nbuilder.add_node(\"finalize_answer\", finalize_answer)\\nThe add_node()\\nmethod takes the first argument as the node’s name and the second argument as the callable that is executed when the node runs.\\nGenerally, this callable can be a plain function, an async function, a LangChain Runnable\\n, or even another compiled StateGraph.\\nIn our specific case:\\ngenerate_query\\ngenerates search queries based on the user’s question.web_search\\nperforms web research using the native Google Search API tool.reflection\\nidentifies knowledge gaps and generates potential follow-up queries.finalize_answer\\nfinalizes the research summary.\\nWe will examine the detailed implementation of those functions later.\\nOk, now that we have the nodes defined, the next step is to add edges to connect them and define execution order:\\nfrom langgraph.graph import START, END\\n# Set the entrypoint as `generate_query`\\n# This means that this node is the first one called\\nbuilder.add_edge(START, \"generate_query\")\\n# Add conditional edge to continue with search queries in a parallel branch\\nbuilder.add_conditional_edges(\\n\"generate_query\", continue_to_web_research, [\"web_research\"]\\n)\\n# Reflect on the web research\\nbuilder.add_edge(\"web_research\", \"reflection\")\\n# Evaluate the research\\nbuilder.add_conditional_edges(\\n\"reflection\", evaluate_research, [\"web_research\", \"finalize_answer\"]\\n)\\n# Finalize the answer\\nbuilder.add_edge(\"finalize_answer\", END)\\nA couple of things are worth pointing out here:\\n- Notice how those node names we defined earlier (e.g., “generate_query”, “web_research”, etc.) now come in handy—we can reference them directly in our edge definitions.\\n- We see that two types of edges are used, i.e., the static edge and the conditional edge.\\n- When\\nbuilder.add_edge()\\nis used, a direct, unconditional connection between two nodes is created. In our case,builder.add_edge(\"web_research\", \"reflection\")\\nbasically means that after web research is completed, the flow will always move to the reflection step. - On the other hand, when\\nbuilder.add_conditional_edges()\\nis used, the flow may jump to different branches at runtime. We need three key arguments when creating a conditional edge: the source node, a routing function, and a list of possible destination nodes. The routing function examines the current state and returns the name of the next node to visit. For example, theevaluate_research()\\nfunction determines whether the agent needs more research (in that case, go to the\"web_research\"\\nnode) or if the information is already sufficient that the agent can finalize the answer (go to the\"finalize_answer\" node\\n).\\nBut why do we need a conditional edge between “generate_query” and “web_research”? Shouldn’t it be a static edge since we always want to search after generating queries? Good catch! That actually has something to do with how LangGraph enables parallelization. We will discuss that later in-depth.\\n- We also notice two special nodes:\\nSTART\\nandEND\\n. These are LangGraph’s built-in entry and exit points. Every graph needs exactly one starting point (where execution begins), but can have multiple ending points (where execution terminates).\\nFinally, it’s time to put everything together and compile the graph into an executable agent:\\ngraph = builder.compile(name=\"pro-search-agent\")\\nAnd that’s it! We’ve successfully translated our flowchart into a LangGraph implementation.\\n🎁 Bonus Read: Why Do Graphs Truly Shine?\\nBeyond being a natural fit for nonlinear workflows, LangGraph’s node/edge/graph representation brings several additional practical benefits that make building and managing agents easy in the real world:\\n- Fine-grained control & observability. Because every node/edge has its own identity, you can easily checkpoint your progress and examine under the hood when something unexpected happens. This makes debugging and evaluation simple.\\n- Modularity & reuse. You can bundle individual steps into reusable subgraphs, just like Lego bricks. Talking about software best practices in action.\\n- Parallel paths. When parts of your workflow are independent, graphs easily let them run concurrently. Obviously, this helps address latency issues and makes your system more robust to faults, which is especially critical when your pipelines are complex.\\n- Easily visualizable. Whether it’s debugging or presenting the approach, it’s always nice to be able to see the workflow logic. Graphs are just natural for visualization.\\n📌Key takeaways\\nLet’s recap what we’ve covered in this foundational section:\\n- LangGraph uses graphs to describe the agentic workflow, as graphs elegantly handle branching, looping, and other nonlinear procedures.\\n- In LangGraph, nodes represent processing steps and edges define transitions between steps.\\n- LangGraph implements two types of edges: static edges and conditional edges. When you have fixed transitions between nodes, use static edges. If the transition may change in runtime based on dynamic decision, use conditional edges.\\n- Building a graph in LangGraph is simple. You first create a StateGraph, then add nodes (with their functions), connect them with edges. Finally, you compile the graph. Done!\\nNow that we understand the basic structure, you’re probably wondering: how does information flow between these nodes? This brings us to one of LangGraph’s most important concepts: state management.\\nLet’s check that out.\\n2. The Agent’s Memory — How Nodes Share Information with State\\n🎯 The problem\\nAs our agent walks through the graph we defined earlier, it needs to keep track of things it has generated/learned. For example:\\n- The original question from the user.\\n- The list of search queries it has generated.\\n- The content it has retrieved from the web.\\n- Its own internal reflections about whether the gathered information is sufficient.\\n- The final, polished answer.\\nSo, how should we maintain that information so that our nodes do not work in isolation but instead collaborate and build upon each other’s work?\\n💡 LangGraph’s solution\\nThe LangGraph way of solving this problem is by introducing a central state object, a shared whiteboard that every node in the graph can look at and write on.\\nHere’s how it works:\\n- When a node is executed, it receives the current state of the graph.\\n- The node performs its task (e.g., calls an LLM, runs a tool) using information from the state.\\n- The node then returns a dictionary containing only the parts of the state it wants to update or add.\\n- LangGraph then takes this output and automatically merges it into the main state object, before passing it to the next node.\\nSince the state passing and merging are handled at the framework level by LangGraph, individual nodes don’t need to worry about how to access or update shared data. They just need to focus on their specific task logic.\\nAlso, this pattern makes your agent workflows highly modular. You can easily add, remove, or reorder nodes without breaking the state flow.\\n</> Let’s see some code!\\nRemember this line from the last section?\\n# Create our Agent Graph\\nbuilder = StateGraph(OverallState, config_schema=Configuration)\\nWe mentioned that OverallState\\ndefines the agent’s memory, but does not yet show how exactly it is implemented. Now it’s a good time to open the black box.\\nIn the repo, OverallState\\nis defined inagent/state.py\\n:\\nfrom typing import TypedDict, Annotated, List\\nfrom langgraph.graph.message import add_messages\\nimport operator\\nclass OverallState(TypedDict):\\nmessages: Annotated[list, add_messages]\\nsearch_query: Annotated[list, operator.add]\\nweb_research_result: Annotated[list, operator.add]\\nsources_gathered: Annotated[list, operator.add]\\ninitial_search_query_count: int\\nmax_research_loops: int\\nresearch_loop_count: int\\nreasoning_model: str\\nEssentially, we can see that the so-called state is a TypedDict\\nthat serves as a contract. It defines every field your workflow cares about and how those fields should be merged when multiple nodes write to them. Let’s break that down:\\n- Field purposes:\\nmessages\\nstores conversation history,search_query\\n,web_search_result\\n, andsource_gathered\\ntrack the agent’s research process. The other fields control agent behavior by setting limits and tracking progress. - The Annotated pattern: We see some fields use\\nAnnotated[list, add_messages]\\norAnnotated[list, operator.add]\\n. This is meant to tell LangGraph how to do the merge update when multiple nodes modify the same field. Specifically,add_messages\\nis LangGraph’s built-in function for intelligently merging conversation messages, whileoperator.add\\nconcatenates lists when nodes add new items. - Merge behavior: Fields like\\nresearch_loop_count: int\\nsimply replace the old value when updated. Annotated fields, on the other hand, are cumulative. They build up over time as different nodes dump information into it.\\nWhile OverallState\\nserves as the global memory, probably it is better to also define smaller, node-specific states to act as a clear “API contract” for what a node needs and produces. After all, it is often the case that one specific node will not require all the information from the entire OverallState\\n, nor modify all the content in OverallState\\n.\\nThis is exactly what LangGraph did.\\nInagent/state.py\\n, besides defining OverallState\\n, three other states are also defined:\\nclass ReflectionState(TypedDict):\\nis_sufficient: bool\\nknowledge_gap: str\\nfollow_up_queries: Annotated[list, operator.add]\\nresearch_loop_count: int\\nnumber_of_ran_queries: int\\nclass QueryGenerationState(TypedDict):\\nquery_list: list[Query]\\nclass WebSearchState(TypedDict):\\nsearch_query: str\\nid: str\\nThose states are used by the nodes in the following way (agent/graph.py\\n):\\nfrom agent.state import (\\nOverallState,\\nQueryGenerationState,\\nReflectionState,\\nWebSearchState,\\n)\\ndef generate_query(\\nstate: OverallState,\\nconfig: RunnableConfig\\n) -> QueryGenerationState:\\n# ...Some logic to generate search queries...\\nreturn {\"query_list\": result.query}\\ndef continue_to_web_research(\\nstate: QueryGenerationState\\n):\\n# ...Some logic to send out multiple search queries...\\ndef web_research(\\nstate: WebSearchState,\\nconfig: RunnableConfig\\n) -> OverallState:\\n# ...Some logic to performs web research...\\nreturn {\\n\"sources_gathered\": sources_gathered,\\n\"search_query\": [state[\"search_query\"]],\\n\"web_research_result\": [modified_text],\\n}\\ndef reflection(\\nstate: OverallState,\\nconfig: RunnableConfig\\n) -> ReflectionState:\\n# ...Some logic to reflect on the results...\\nreturn {\\n\"is_sufficient\": result.is_sufficient,\\n\"knowledge_gap\": result.knowledge_gap,\\n\"follow_up_queries\": result.follow_up_queries,\\n\"research_loop_count\": state[\"research_loop_count\"],\\n\"number_of_ran_queries\": len(state[\"search_query\"]),\\n}\\ndef evaluate_research(\\nstate: ReflectionState,\\nconfig: RunnableConfig,\\n) -> OverallState:\\n# ...Some logic to determine the next step in the research flow...\\ndef finalize_answer(\\nstate: OverallState,\\nconfig: RunnableConfig) -> OverallState:\\n# ...Some logic to finalize the research summary...\\nreturn {\\n\"messages\": [AIMessage(content=result.content)],\\n\"sources_gathered\": unique_sources,\\n}\\nTake thereflection\\nnode as an example: It reads from the OverallState\\nbut returns a dictionary that matches the ReflectionState\\ncontract. Afterward, LangGraph will handle the job of merging them into the main OverallState\\n, making them available for the next nodes in the graph.\\n🎁 Bonus Read: Where Did My State Go?\\nA common confusion when working with LangGraph is how OverallState\\nand these smaller, node-specific states interact. Let’s clear that confusion here.\\nThe crucial mental model we need to have is this: there is only one state dictionary at runtime, the OverallState\\n.\\nNode-specific TypedDict\\ns are not extra runtime data stores. Instead, they are just typed “views” onto the one underlying dictionary (OverallState\\n), that temporarily zoom in on the parts a node should see or produce. The purpose of their existence is that the type checker and the LangGraph runtime can enforce clear contracts.\\nBefore a node runs, LangGraph can use its type hints to create a “slice” of the OverallState\\ncontaining only the inputs that the node needs.\\nThe node runs its logic and returns its small, specific output dictionary (e.g., a ReflectionState\\ndict).\\nLangGraph takes the returned dictionary and runs OverallState.update(return_dict)\\n. If any keys were defined with an aggregator (like operator.add\\n), that logic is applied. The updated OverallState\\nis then passed to the next node.\\nSo why has LangGraph embraced this two-level state definition? Besides enforcing a clear contract for the node and making node operations self-documenting, there are two other benefits also worth mentioning:\\n- Drop-in reusability: Because a node only advertises the small slice of state it needs and produces, it becomes a modular, plug-and-play component. For example, a\\ngenerate_query\\nnode that only needs{user_query}\\nfrom the state and outputs{queries}\\ncan be dropped into another, completely different graph, so long as that graph’sOverallState\\ncan provide auser_query\\n. If the node were coded against the entire global state (i.e., typed withOverallState\\nfor both its input and output), you can easily break the workflow if you rename any unrelated key. This modularity is quite essential for building complex systems. - Efficiency in parallel flows: Imagine our agent needs to run 10 web searches simultaneously. If we are using a node-specific state as a small payload, we then just need to send the search query to each parallel branch. This is way more efficient than sending a copy of the entire agent memory (remember the full chat history is also stored in\\nOverallState\\n!) to all ten branches. This way, we can dramatically cut down on memory and serialization overhead.\\nSo what does this mean for us in practice?\\n- ✔ Declare in\\nOverallState\\nevery key that needs to persist or to be visible to multiple different nodes. - ✔ Make the node-specific states as small as possible. They should contain only the fields that the node is responsible for producing.\\n- ✔ Every key you write must be declared in some state schema; otherwise, LangGraph raises\\nInvalidUpdateError\\nwhen the node tries to write it.\\n📌Key takeaways\\nLet’s recap what we’ve covered in this section:\\n- LangGraph maintains states at two levels: At the global level, there is the OverallState object that serves as the central memory. At the individual node level, small, TypedDict-based objects store node-specific inputs/outputs. This keeps the state management clean and organized.\\n- After each step, nodes would return minimal output dicts, which is then merged back into the central memory (\\nOverallState\\n). This merging is done according to your custom rules (e.g.,operator.add\\nfor lists). - Nodes are self-contained and modular. You can easily resue them like building blocks to create new workflows.\\nNow we’ve understood the graph’s structure and how state flows through it, but what happens inside each node? Let’s now turn to the node operations.\\n3. Node Operations — Where The Real Work Happens\\nOur graph can route messages and hold state, but inside each node, we still need to:\\n- Make sure the LLM outputs the right format.\\n- Call external APIs.\\n- Run multiple searches in parallel.\\n- Decide when to stop the loop.\\nLuckily, LangGraph has your back with several solid approaches for tackling these challenges. Let’s meet them one by one, each through a slice of our working codebase.\\n3.1 Structured output\\n🎯 The problem\\nGetting an LLM to return a JSON object is easy, but parsing free-text JSON is just unreliable in practice. As soon as LLMs use a different phrase, add unexpected formatting, or change the key order, our workflow can easily go off the rails. In short, we need guaranteed, validatable output structures at each processing step.\\n💡 LangGraph’s solution\\nWe constrain the LLM to generate output that conforms to a predefined schema. This can be done by attaching a Pydantic schema to the LLM call using llm.with_structured_output()\\n, which is a helper method that is provided by every LangChain chat-model wrapper (e.g., ChatGoogleGenerativeAI\\n, ChatOpenAI\\n, etc.).\\n</> Let’s see some code!\\nLet’s look at the generate_query\\nnode, whose job is to create a list of search queries. Since we need this list to be a clean Python object, not a messy string, for the next node to parse, it would be a good idea to enforce the output schema, with SearchQueryList\\n(defined in agent/tools_and_schemas.py\\n):\\nfrom typing import List\\nfrom pydantic import BaseModel, Field\\nclass SearchQueryList(BaseModel):\\nquery: List[str] = Field(\\ndescription=\"A list of search queries to be used for web research.\"\\n)\\nrationale: str = Field(\\ndescription=\"A brief explanation of why these queries are relevant to the research topic.\"\\n)\\nAnd here is how this schema is used in the generate_query\\nnode:\\nfrom langchain_google_genai import ChatGoogleGenerativeAI\\nfrom agent.prompts import (\\nget_current_date,\\nquery_writer_instructions,\\n)\\ndef generate_query(\\nstate: OverallState,\\nconfig: RunnableConfig\\n) -> QueryGenerationState:\\n\"\"\"LangGraph node that generates a search queries\\nbased on the User\\'s question.\\nUses Gemini 2.0 Flash to create an optimized search\\nquery for web research based on the User\\'s question.\\nArgs:\\nstate: Current graph state containing the User\\'s question\\nconfig: Configuration for the runnable, including LLM\\nprovider settings\\nReturns:\\nDictionary with state update, including search_query key\\ncontaining the generated query\\n\"\"\"\\nconfigurable = Configuration.from_runnable_config(config)\\n# check for custom initial search query count\\nif state.get(\"initial_search_query_count\") is None:\\nstate[\"initial_search_query_count\"] = configurable.number_of_initial_queries\\n# init Gemini 2.0 Flash\\nllm = ChatGoogleGenerativeAI(\\nmodel=configurable.query_generator_model,\\ntemperature=1.0,\\nmax_retries=2,\\napi_key=os.getenv(\"GEMINI_API_KEY\"),\\n)\\nstructured_llm = llm.with_structured_output(SearchQueryList)\\n# Format the prompt\\ncurrent_date = get_current_date()\\nformatted_prompt = query_writer_instructions.format(\\ncurrent_date=current_date,\\nresearch_topic=get_research_topic(state[\"messages\"]),\\nnumber_queries=state[\"initial_search_query_count\"],\\n)\\n# Generate the search queries\\nresult = structured_llm.invoke(formatted_prompt)\\nreturn {\"query_list\": result.query}\\nHere, llm.with_structured_output(SearchQueryList)\\nwraps the Gemini model with LangChain’s structured-output helper. Under the hood, it uses the model’s preferred structured-output feature (JSON mode for Gemini 2.0 Flash) and automatically parses the reply into a SearchQueryList\\nPydantic instance, so result\\nis already validated Python data.\\nIt’s also interesting to check out the system prompt Google used for this node:\\nquery_writer_instructions = \"\"\"Your goal is to generate sophisticated and\\ndiverse web search queries. These queries are intended for an advanced\\nautomated web research tool capable of analyzing complex results, following\\nlinks, and synthesizing information.\\nInstructions:\\n- Always prefer a single search query, only add another query if the original\\nquestion requests multiple aspects or elements and one query is not enough.\\n- Each query should focus on one specific aspect of the original question.\\n- Don\\'t produce more than {number_queries} queries.\\n- Queries should be diverse, if the topic is broad, generate more than 1 query.\\n- Don\\'t generate multiple similar queries, 1 is enough.\\n- Query should ensure that the most current information is gathered.\\nThe current date is {current_date}.\\nFormat:\\n- Format your response as a JSON object with ALL three of these exact keys:\\n- \"rationale\": Brief explanation of why these queries are relevant\\n- \"query\": A list of search queries\\nExample:\\nTopic: What revenue grew more last year apple stock or the number of people\\nbuying an iphone\\n```json\\n{{\\n\"rationale\": \"To answer this comparative growth question accurately,\\nwe need specific data points on Apple\\'s stock performance and iPhone sales\\nmetrics. These queries target the precise financial information needed:\\ncompany revenue trends, product-specific unit sales figures, and stock price\\nmovement over the same fiscal period for direct comparison.\",\\n\"query\": [\"Apple total revenue growth fiscal year 2024\", \"iPhone unit\\nsales growth fiscal year 2024\", \"Apple stock price growth fiscal year 2024\"],\\n}}\\n```\\nContext: {research_topic}\"\"\"\\nWe see some prompt engineering best practices in action, like defining the model’s role, specifying constraints, providing an example for illustration, etc.\\n3.2 Tool calling\\n🎯 The problem\\nFor our research agent to succeed, it needs up-to-date information from the web. To realize that, it needs a “tool” to search the web.\\n💡 LangGraph’s solution\\nNodes can execute tools. These can be native LLM tool-calling features (like in Gemini) or integrated through LangChain’s tool abstractions. Once the tool-calling results are gathered, they can be placed back into the agent’s state.\\n</> Let’s see some code!\\nFor the tool-calling usage pattern, let’s look at the web_research\\nnode. This node uses Gemini’s native tool-calling feature to perform Google searches. Notice how the tool is specified directly in the model’s configuration.\\nfrom langchain_google_genai import ChatGoogleGenerativeAI\\nfrom agent.prompts import (\\nweb_searcher_instructions,\\n)\\nfrom agent.utils import (\\nget_citations,\\ninsert_citation_markers,\\nresolve_urls,\\n)\\ndef web_research(\\nstate: WebSearchState,\\nconfig: RunnableConfig\\n) -> OverallState:\\n\"\"\"LangGraph node that performs web research using the native Google\\nSearch API tool.\\nExecutes a web search using the native Google Search API tool in\\ncombination with Gemini 2.0 Flash.\\nArgs:\\nstate: Current graph state containing the search query and\\nresearch loop count\\nconfig: Configuration for the runnable, including search API settings\\nReturns:\\nDictionary with state update, including sources_gathered,\\nresearch_loop_count, and web_research_results\\n\"\"\"\\n# Configure\\nconfigurable = Configuration.from_runnable_config(config)\\nformatted_prompt = web_searcher_instructions.format(\\ncurrent_date=get_current_date(),\\nresearch_topic=state[\"search_query\"],\\n)\\n# Uses the google genai client as the langchain client doesn\\'t\\n# return grounding metadata\\nresponse = genai_client.models.generate_content(\\nmodel=configurable.query_generator_model,\\ncontents=formatted_prompt,\\nconfig={\\n\"tools\": [{\"google_search\": {}}],\\n\"temperature\": 0,\\n},\\n)\\n# resolve the urls to short urls for saving tokens and time\\nresolved_urls = resolve_urls(\\nresponse.candidates[0].grounding_metadata.grounding_chunks, state[\"id\"]\\n)\\n# Gets the citations and adds them to the generated text\\ncitations = get_citations(response, resolved_urls)\\nmodified_text = insert_citation_markers(response.text, citations)\\nsources_gathered = [item for citation in citations for item in citation[\"segments\"]]\\nreturn {\\n\"sources_gathered\": sources_gathered,\\n\"search_query\": [state[\"search_query\"]],\\n\"web_research_result\": [modified_text],\\n}\\nThe LLM sees the Google Search\\ntool and understands that it can use the tool to fulfill the prompt. A key benefit of this native integration is the grounding_metadata\\nreturned with the response. That metadata contains grounding chunks — essentially, snippets of the answer paired with the URL that justified them. This basically gives us citations for free.\\n3.3 Conditional routing\\n🎯 The problem\\nAfter the initial research, how does the agent know whether to stop or continue? We need a control mechanism to create a research loop that can terminate itself.\\n💡 LangGraph’s solution\\nConditional routing is handled by a special type of node: instead of returning state, this node returns the name of the next node to visit. Effectively, this node implements a routing function that inspects the current state and makes a decision regarding how to direct the traffic within the graph.\\n</> Let’s see some code!\\nThe evaluate_research\\nnode is our agent’s decision-maker. It checks the is_sufficient\\nflag set by the reflection\\nnode and compares the current research_loop_count\\nvalue against a pre-configured maximum threshold value.\\ndef evaluate_research(\\nstate: ReflectionState,\\nconfig: RunnableConfig,\\n) -> OverallState:\\n\"\"\"LangGraph routing function that determines the next step in the\\nresearch flow.\\nControls the research loop by deciding whether to continue gathering\\ninformation or to finalize the summary based on the configured maximum\\nnumber of research loops.\\nArgs:\\nstate: Current graph state containing the research loop count\\nconfig: Configuration for the runnable, including max_research_loops\\nsetting\\nReturns:\\nString literal indicating the next node to visit\\n(\"web_research\" or \"finalize_summary\")\\n\"\"\"\\nconfigurable = Configuration.from_runnable_config(config)\\nmax_research_loops = (\\nstate.get(\"max_research_loops\")\\nif state.get(\"max_research_loops\") is not None\\nelse configurable.max_research_loops\\n)\\nif state[\"is_sufficient\"] or state[\"research_loop_count\"] >= max_research_loops:\\nreturn \"finalize_answer\"\\nelse:\\nreturn [\\nSend(\\n\"web_research\",\\n{\\n\"search_query\": follow_up_query,\\n\"id\": state[\"number_of_ran_queries\"] + int(idx),\\n},\\n)\\nfor idx, follow_up_query in enumerate(state[\"follow_up_queries\"])\\n]\\nIf the condition to stop is met, it returns the string \"finalize_answer\"\\n, and LangGraph proceeds to that node. If not, it returns a new list of Send\\nobjects containing the follow_up_queries\\n, which spins up another parallel wave of web research, continuing the loop.\\nSend\\nobject…What is it then?\\nWell, it is LangGraph’s way of triggering parallel execution. Let’s turn to that now.\\n3.4 Parallel processing\\n🎯 The problem\\nTo answer the user’s query as comprehensively as possible, we would need our generate_query\\nnode to produce multiple search queries. However, we don’t want to run those search queries one by one, as it would be very slow and inefficient. What we want is to execute the web searches for all queries concurrently.\\n💡 LangGraph’s solution\\nTo trigger parallel execution, a node can return a list of Send\\nobjects. Send\\nis a special directive that tells the LangGraph scheduler to dispatch these tasks to the specified node (e.g.,\"web_research\"\\n) concurrently, each with its own piece of state.\\n</> Let’s see some code!\\nTo enable the parallel search, Google’s implementation introduces the continue_to_web_research\\nnode to act as a dispatcher. It takes the query_list\\nfrom the state and creates a separate Send\\ntask for each query.\\nfrom langgraph.types import Send\\ndef continue_to_web_research(\\nstate: QueryGenerationState\\n):\\n\"\"\"LangGraph node that sends the search queries to the web research node.\\nThis is used to spawn n number of web research nodes, one for each\\nsearch query.\\n\"\"\"\\nreturn [\\nSend(\"web_research\", {\"search_query\": search_query, \"id\": int(idx)})\\nfor idx, search_query in enumerate(state[\"query_list\"])\\n]\\nAnd that’s all the code you need. The magic lives in what happens after this node returns.\\nWhen LangGraph receives this list, it’s smart enough not to simply loop through it. In fact, it triggers a sophisticated fan-out/fan-in process under the hood to handle things concurrently:\\nTo begin with, each Send\\nobject carries only the tiny payload you gave it ({\"search_query\": ..., \"id\": ...}\\n), not the entire OverallState\\n. The purpose here is to have fast serialization.\\nThen, the graph scheduler spins off an asyncio\\ntask for every item in the list. This concurrency happens automatically, you as the workflow builder don’t need to worry anything about writing async def\\nor managing a thread pool.\\nFinally, after all the parallel web_research\\nbranches are completed, their individually returned dictionaries are automatically merged back into the main OverallState\\n. Remember the Annotated[list, operator.add]\\nwe discussed in the beginning? Now it becomes crucial: fields defined with this type of reducer, like sources_gathered\\n, will have their results concatenated into a single list.\\nYou may want to ask: what happens if one of the parallel searches fails or times out? This is exactly why we added a custom id\\nto each Send\\npayload. This ID flows directly into the trace logs, allowing you to pinpoint and debug the exact branch that failed.\\nIf you remember from earlier, we have the following line in our graph definition:\\n# Add conditional edge to continue with search queries in a parallel branch\\nbuilder.add_conditional_edges(\\n\"generate_query\", continue_to_web_research, [\"web_research\"]\\n)\\nYou might be wondering: why do we need to declare continue_to_web_research\\nnode as part of a conditional edge?\\nThe crucial thing to realize is that: continue_to_web_research\\nisn’t just another step in the pipeline—it’s a routing function.\\nThe generate_query\\nnode can return zero queries (when the user asks something trivial) or twenty. A static edge would force the workflow to invoke web_research\\nexactly once, even if there’s nothing to do. By implementing as a conditional edge continue_to_web_research\\ndecides at runtime, whether to dispatch—and, thanks to Send\\n, how many parallel branches to spawn. If continue_to_web_research\\nreturns an empty list, LangGraph simply doesn’t follow the edge. That saves the round-trip to the search API.\\nFinally, this is again the software engineering best practice in action: generate_query\\nfocuses on what to search, continue_to_web_research\\non whether and how to search, and web_research\\non doing the search, a clean separation of concerns.\\n3.5 Configuration management\\n🎯 The problem\\nFor nodes to properly do their jobs, they need to know, for example:\\n- Which LLM to use with what parameter settings (e.g., temperature)?\\n- How many initial search queries should be generated?\\n- What’s the cap on total research loops and on per-run concurrency?\\n- And many others…\\nIn short, we need a clean, centralized way to manage these settings without cluttering our core logic.\\n💡 LangGraph’s Solution\\nLangGraph solves this by passing a single, standardized config\\ninto every node that needs it. This object acts as a universal container for run-specific settings.\\nInside the node, LangGraph then uses a custom, typed helper class to intelligently parse this config\\nobject. This helper class implements a clear hierarchy for fetching values:\\n- It first looks for overrides passed in the\\nconfig\\nobject for the current run. - If not found, it falls back to checking for environment variables.\\n- If still not found, it uses the defaults defined directly in this helper class.\\n</> Let’s see some code!\\nLet’s look at the implementation of the reflection\\nnode to see it in action.\\ndef reflection(\\nstate: OverallState,\\nconfig: RunnableConfig\\n) -> ReflectionState:\\n\"\"\"LangGraph node that identifies knowledge gaps and generates\\npotential follow-up queries.\\nAnalyzes the current summary to identify areas for further research\\nand generates potential follow-up queries. Uses structured output to\\nextract the follow-up query in JSON format.\\nArgs:\\nstate: Current graph state containing the running summary and\\nresearch topic\\nconfig: Configuration for the runnable, including LLM provider\\nsettings\\nReturns:\\nDictionary with state update, including search_query key containing\\nthe generated follow-up query\\n\"\"\"\\nconfigurable = Configuration.from_runnable_config(config)\\n# Increment the research loop count and get the reasoning model\\nstate[\"research_loop_count\"] = state.get(\"research_loop_count\", 0) + 1\\nreasoning_model = state.get(\"reasoning_model\") or configurable.reasoning_model\\n# Format the prompt\\ncurrent_date = get_current_date()\\nformatted_prompt = reflection_instructions.format(\\ncurrent_date=current_date,\\nresearch_topic=get_research_topic(state[\"messages\"]),\\nsummaries=\"\\\\n\\\\n---\\\\n\\\\n\".join(state[\"web_research_result\"]),\\n)\\n# init Reasoning Model\\nllm = ChatGoogleGenerativeAI(\\nmodel=reasoning_model,\\ntemperature=1.0,\\nmax_retries=2,\\napi_key=os.getenv(\"GEMINI_API_KEY\"),\\n)\\nresult = llm.with_structured_output(Reflection).invoke(formatted_prompt)\\nreturn {\\n\"is_sufficient\": result.is_sufficient,\\n\"knowledge_gap\": result.knowledge_gap,\\n\"follow_up_queries\": result.follow_up_queries,\\n\"research_loop_count\": state[\"research_loop_count\"],\\n\"number_of_ran_queries\": len(state[\"search_query\"]),\\n}\\nJust one line of boilerplate is required in the node:\\nconfigurable = Configuration.from_runnable_config(config)\\nThere are quite a few “config-ish” terms floating around. Let’s unpack them one by one, starting with Configuration\\n:\\nimport os\\nfrom pydantic import BaseModel, Field\\nfrom typing import Any, Optional\\nfrom langchain_core.runnables import RunnableConfig\\nclass Configuration(BaseModel):\\n\"\"\"The configuration for the agent.\"\"\"\\nquery_generator_model: str = Field(\\ndefault=\"gemini-2.0-flash\",\\nmetadata={\\n\"description\": \"The name of the language model to use for the agent\\'s query generation.\"\\n},\\n)\\nreflection_model: str = Field(\\ndefault=\"gemini-2.5-flash-preview-04-17\",\\nmetadata={\\n\"description\": \"The name of the language model to use for the agent\\'s reflection.\"\\n},\\n)\\nanswer_model: str = Field(\\ndefault=\"gemini-2.5-pro-preview-05-06\",\\nmetadata={\\n\"description\": \"The name of the language model to use for the agent\\'s answer.\"\\n},\\n)\\nnumber_of_initial_queries: int = Field(\\ndefault=3,\\nmetadata={\"description\": \"The number of initial search queries to generate.\"},\\n)\\nmax_research_loops: int = Field(\\ndefault=2,\\nmetadata={\"description\": \"The maximum number of research loops to perform.\"},\\n)\\n@classmethod\\ndef from_runnable_config(\\ncls, config: Optional[RunnableConfig] = None\\n) -> \"Configuration\":\\n\"\"\"Create a Configuration instance from a RunnableConfig.\"\"\"\\nconfigurable = (\\nconfig[\"configurable\"] if config and \"configurable\" in config else {}\\n)\\n# Get raw values from environment or config\\nraw_values: dict[str, Any] = {\\nname: os.environ.get(name.upper(), configurable.get(name))\\nfor name in cls.model_fields.keys()\\n}\\n# Filter out None values\\nvalues = {k: v for k, v in raw_values.items() if v is not None}\\nreturn cls(**values)\\nThis is the custom helper class we mentioned earlier. You can see Pydantic is heavily used to define all the parameters for the agent. One thing to notice is that this class also defines an alternative constructor method from_runnable_config()\\n. This constructor method creates a Configuration\\ninstance by pulling values from different sources while enforcing the overriding hierarchy we discussed in “💡 LangGraph’s Solution” above.\\nconfig\\nis the input to from_runnable_config()\\nmethod. Technically, it’s a RunnableConfig\\ntype, but it’s really just a dictionary with optional metadata. In LangGraph, it’s mainly used as a structured way to carry contextual information across the graph. For example, it can carry things like tags, tracing options, and — most importantly—a nested dictionary of overrides under the \"configurable\"\\nkey.\\nFinally, by calling in every node:\\nconfigurable = Configuration.from_runnable_config(config)\\nwe create an instance of the Configuration\\nclass by combining data from three sources: first, the config[\"configurable\"]\\n, then environment variables, and finally the class defaults. So configurable\\nis a fully initialized, ready-to-use object that gives the node access to all relevant settings, such as configurable.reflection_model\\n.\\nThere is a bug in Google’s original code (both in reflection node & finalize_answer node):\\nreasoning_model = state.get(\"reasoning_model\") or configurable.reasoning_model\\nHowever,\\nreasoning_model\\nis never defined in the configuration.py. Instead,reflect_model\\nandanswer_model\\nshould be used per configuration.py definitions. Details see PR #46.\\nTo recap: Configuration\\nis the definition, config\\nis the runtime input, and configurable\\nis the result, i.e., the parsed configuration object your node uses.\\n🎁 Bonus Read: What Didn’t We Cover?\\nLangGraph has a lot more to offer than what we can cover in this tutorial. As you build more complex agents, you’ll probably find yourself asking questions like these:\\n1. Can I make my application more responsive?\\nLangGraph supports streaming, so you can output results token by token for a real-time user experience.\\n2. What happens when an API call fails?\\nLangGraph implements retry and fallback mechanisms to handle errors.\\n3. How to avoid re-running expensive computations?\\nIf some of your nodes need to conduct expensive processing, you can use LangGraph’s caching mechanism to cache the node outputs. Also, LangGraph supports checkpoints. This feature lets you save your graph’s state and pick up where you left off. This is especially important if you have a long-running process and you want to pause it and resume it later.\\n4. Can I implement human-in-the-loop workflows?\\nYes. LangGraph has built-in support for human-in-the-loop workflows. This enables you to pause the graph and wait for user input or approval before proceeding.\\n5. How can I trace my agent’s behavior?\\nLangGraph integrates natively with LangSmith, which provides detailed traces and observability into your agent’s behaviors with minimal setup.\\n6. How can my agent automatically discover and use new tools?\\nLangGraph supports MCP (Model Context Protocol) integrations. This allows it to auto-discover and use tools that follow this open standard.\\nCheck out the LangGraph official docs for more details.\\n📌Key takeaways\\nLet’s recap what we’ve covered in this section:\\n- Structured output: Use .\\nwith_structured_output\\nto force the AI’s response to fit a specific structure you define. This makes sure you always get clean, reliable data that your downstream steps can easily parse. - Tool calling: You can embed tools in the model calls so that the agent can interact with the outside world.\\n- Conditional routing: This is how you build “choose your own adventure” logic. A node can decide where to go next simply by returning the name of the next node. This way, you can dynamically create loops and decision points, making your agent’s workflow much more intelligent.\\n- Parallel processing: LangGraph allows you to trigger multiple steps to run at the same time. All the heavy lifting of fanning out the jobs and fanning back in to collect the results are automatically handled by LangGraph.\\n- Configuration management: Instead of scattering settings throughout your code, you can use a dedicated Configuration class to manage runtime settings, environment variables, defaults, etc., in one clean, central place.\\n4. Conclusions\\nWe have covered a lot of ground in this post! Now we’ve seen how LangGraph’s core concepts come together to build a real-world research agent, let’s conclude our journey with a few key takeaways:\\n- Graphs naturally describe agentic workflows. Real-world workflows involve loops, branches, and dynamic decisions. LangGraph’s graph-based architecture (nodes, edges, and state) provides a clean and intuitive way to represent and manage this complexity.\\n- State is the agent’s memory. The central\\nOverallState\\nobject is a shared whiteboard that every node in the graph can look at and write on. Together with node-specific state schemas, they create the agent’s memory system. - Nodes are modular components that are reusable. In LangGraph, you should build nodes with clear responsibilities, e.g., generating queries, calling tools, or routing logic. This makes the agentic system easier to test, maintain, and extend.\\n- Control is in your hands. In LangGraph, you can direct the logical flow with conditional edges, enforce data reliability with structured outputs, use centralized configuration to tune parameters globally, or use\\nSend\\nto achieve parallel execution of tasks. Their combination gives you the power to build smart, efficient, and reliable agents.\\nNow with all the knowledge you have about LangGraph, what do you want to build next?'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_url(\"https://towardsdatascience.com/langgraph-101-lets-build-a-deep-research-agent/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e0fc49",
   "metadata": {},
   "source": [
    "#### Vector Store\n",
    "app/tools/vectorstore.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb57c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import chromadb\n",
    "from project_files.configs import GEMINI_API_KEY, VECTOR_DB_DIR, MODEL_NAME\n",
    "\n",
    "def get_vectorstore():\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=MODEL_NAME, google_api_key=GEMINI_API_KEY)\n",
    "    client = chromadb.PersistentClient(path=VECTOR_DB_DIR)\n",
    "    return client, embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645143d2",
   "metadata": {},
   "source": [
    "#### Moderation Tool\n",
    "app/tools/moderation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f66e7cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_toxicity(text):\n",
    "    \"\"\"Basic keyword filter (extend with LLM for production).\"\"\"\n",
    "    bad_words = [\"hate\", \"violence\", \"racist\"]\n",
    "    return any(bad in text.lower() for bad in bad_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c7566a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_toxicity(\"I hate this product, it's terrible!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7b42ba",
   "metadata": {},
   "source": [
    "#### Voice Profile Loader\n",
    "app/tools/voice_profile.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "606168d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from project_files.configs import VOICE_PROFILE_PATH\n",
    "\n",
    "def load_voice_profile():\n",
    "    df = pd.read_csv(VOICE_PROFILE_PATH)\n",
    "    all_text = \" \".join(df[\"post_text\"].dropna().tolist())\n",
    "    return all_text[:3000]  # Keep a manageable token count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f467470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Just published my latest article on AI in healthcare! Excited to hear your thoughts. #AI #Healthcare Data storytelling is key. Always visualize your insights clearly. #DataScience #Visualization Had a fantastic discussion on MLOps best practices today. Key takeaway: automate testing! #MLOps #MachineLearning Exploring RAG (Retrieval-Augmented Generation) techniques for enterprise knowledge management. #RAG #AI Remember: metrics are only valuable if they guide action. Focus on impact, not vanity. #Analytics #Business Thrilled to announce our team's latest project in NLP! More updates soon. #NLP #Innovation Networking is not about collecting contacts, it's about building relationships. #CareerGrowth #LinkedInTips Python tips: list comprehensions can simplify your code and improve readability. #Python #Tips Deep learning models are powerful, but understanding your data is still crucial. #MachineLearning #Data Agile methodology works best when the whole team is aligned on goals and priorities. #Agile #ProjectManagement Had a great mentoring session today! Always love helping others grow their data skills. #Mentorship #DataScience Data privacy is not optional. Make sure to follow best practices in handling sensitive information. #Privacy #DataEthics Visualization hack: color your charts to highlight the key message, not distract from it. #DataViz #Tips Generative AI is changing the way we create content. Always stay updated with latest trends. #GenAI #Innovation Remember to test your models on real-world scenarios, not just training data. #ML #BestPractices Collaboration tools like Notion and Slack improve workflow if used correctly. #Productivity #Tools Data pipelines are the backbone of analytics. Always ensure reliability and monitoring. #DataEngineering #Analytics Storytelling in presentations is more impactful than just numbers. #PresentationSkills #DataStorytelling Today I experimented with embedding-based search for our internal knowledge base. Exciting results! #RAG #Search Continuous learning is key in tech. Take small steps every day to improve skills. #CareerGrowth #Learning The quality of data dictates the quality of insights. Focus on data cleaning before modeling. #DataQuality #Analytics Sharing knowledge is as important as creating it. Blog, write posts, speak at meetups. #KnowledgeSharing #Community Experimentation culture is vital. Fail fast, learn faster. #Innovation #TechCulture A/B testing is powerful, but always define success metrics before starting. #Experimentation #Analytics Excited to join the AI workshop next week! Always looking to learn and network. #AI #Workshop\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_voice_profile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27770bbb",
   "metadata": {},
   "source": [
    "#### Agents Layer\n",
    "Planner\n",
    "\n",
    "app/agents/planner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5489cf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from project_files.configs import GEMINI_API_KEY, MODEL_NAME\n",
    "\n",
    "def create_brief(topic, audience=\"Data professionals\"):\n",
    "    llm = ChatGoogleGenerativeAI(model=MODEL_NAME, google_api_key=GEMINI_API_KEY)\n",
    "    prompt = f\"\"\"\n",
    "    Create a LinkedIn content brief for the topic: {topic}.\n",
    "    Audience: {audience}\n",
    "    Include: angle, 3 key points, suggested title, desired outcome.\n",
    "    \"\"\"\n",
    "    return llm.invoke(prompt).content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b7944ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s a LinkedIn content brief for the topic \"AI Engineer vs. Data Scientist,\" tailored for data professionals:\\n\\n---\\n\\n## LinkedIn Content Brief: AI Engineer vs. Data Scientist\\n\\n**Audience:** Data professionals (Data Scientists, ML Engineers, Data Analysts, Data Engineers, Technical Leads, Hiring Managers in data-driven organizations)\\n\\n---\\n\\n**Angle:**\\n**\"Beyond the Buzzwords: Demystifying the distinct yet highly complementary roles of AI Engineers and Data Scientists to help data professionals navigate career paths, optimize team structures, and understand the modern AI/ML landscape.\"**\\n\\nThis angle aims to clarify confusion, highlight the value of both roles, and provide actionable insights for career development and team building.\\n\\n---\\n\\n**3 Key Points:**\\n\\n1.  **Core Focus & Deliverables:**\\n    *   **Data Scientist:** Primarily focused on *discovery, insights, and model prototyping*. They identify business problems, explore data, develop hypotheses, build experimental models, and communicate findings to drive strategic decisions. Their output is often a validated model prototype and actionable insights.\\n    *   **AI Engineer:** Primarily focused on *delivery, scalability, and productionization*. They take validated models and integrate them into robust, scalable, and maintainable production systems. This involves MLOps, infrastructure, software engineering best practices, and ensuring model performance in real-world environments. Their output is a deployed, operational AI/ML system.\\n\\n2.  **Key Skillsets & Toolkits:**\\n    *   **Data Scientist:** Strong in statistics, machine learning algorithms, experimental design, data visualization, business acumen, and storytelling. Common tools include Python (Pandas, NumPy, Scikit-learn), R, SQL, and BI tools.\\n    *   **AI Engineer:** Strong in software engineering principles, distributed systems, MLOps (CI/CD for ML), cloud platforms (AWS, Azure, GCP), deep learning frameworks (TensorFlow, PyTorch), and containerization (Docker, Kubernetes). They often have a deeper understanding of computational efficiency and system architecture.\\n\\n3.  **Synergy & Team Dynamics:**\\n    *   These roles are not in competition but are **highly collaborative and essential** for successful AI initiatives. A modern, high-performing data team leverages the strengths of both. Data Scientists identify *what* problems to solve and *which* models work, while AI Engineers figure out *how* to make those models work reliably and at scale. Effective communication and shared goals are crucial for translating prototypes into impactful production systems.\\n\\n---\\n\\n**Suggested Title Options:**\\n\\n*   **AI Engineer vs. Data Scientist: Decoding the Modern Data Team**\\n*   **Which Path is Yours? AI Engineer vs. Data Scientist Explained**\\n*   **Beyond the Hype: AI Engineer vs. Data Scientist - A Clear Breakdown**\\n*   **AI Engineer vs. Data Scientist: Understanding the Evolving Roles in AI**\\n\\n---\\n\\n**Desired Outcome:**\\n\\n*   **For Readers:** Gain clear understanding and differentiation between AI Engineer and Data Scientist roles. Feel more confident about their career trajectory or potential next steps. Engage in thoughtful discussion in the comments.\\n*   **For Post Creator:** Positioned as a thought leader in the data and AI space. Generate meaningful engagement (likes, comments, shares, saves) from the target audience. Spark conversations that clarify confusion and highlight the importance of both roles.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_brief(\"AI engineer vs data scientist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce998304",
   "metadata": {},
   "source": [
    "#### Researcher\n",
    "app/agents/researcher.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d8f8457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from app.tools.search_tool import search_web\n",
    "#from app.tools.scrape_tool import scrape_url\n",
    "\n",
    "def gather_evidence(topic):\n",
    "    results = search_web_ddg(topic)\n",
    "    # evidence = []\n",
    "    # for r in results:\n",
    "    #     text = scrape_url(r[\"url\"])\n",
    "    #     if text:\n",
    "    #         evidence.append({\"url\": r[\"url\"], \"snippet\": text[:500]})\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "73127c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amber/Documents/Learnings_2024/Linkdein_content_generator/linkdein_genai_env/lib/python3.10/site-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Aug 6, 2025 · Data Scientist vs. AI Engineer This article aims to delineate the distinctions and overlaps between Data Scientists … Jun 17, 2025 · Confused between becoming an AI Engineer or a Data Scientist? Discover career growth, required skills, salary … Mar 4, 2025 · AI engineer vs. data scientist: What's the difference? AI engineers and data scientists both shape AI projects, … May 20, 2025 · Explore the distinct roles of AI engineers and data scientists, their key differences, and the unique career paths … Mar 12, 2025 · AI engineers focus on building AI-powered applications, while data scientists focus on data analysis and …\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gather_evidence(\"AI engineer vs data scientist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d1a76a",
   "metadata": {},
   "source": [
    "#### Writer\n",
    "app/agents/writer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a024b4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from project_files.configs import GEMINI_API_KEY, MODEL_NAME\n",
    "\n",
    "def draft_posts(topic, evidence, voice_profile):\n",
    "    llm = ChatGoogleGenerativeAI(model=MODEL_NAME, google_api_key=GEMINI_API_KEY)\n",
    "    prompt = f\"\"\"\n",
    "    Write 2 LinkedIn post drafts on \"{topic}\" for tech audience.\n",
    "    Use evidence below and match voice style:\n",
    "    Voice profile: {voice_profile}\n",
    "    Evidence: {evidence}\n",
    "\n",
    "    Output format:\n",
    "    - Variant A: Hook, Body, CTA, Hashtags\n",
    "    - Variant B: Hook, Body, CTA, Hashtags\n",
    "    \"\"\"\n",
    "    return llm.invoke(prompt).content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "56d3d2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amber/Documents/Learnings_2024/Linkdein_content_generator/linkdein_genai_env/lib/python3.10/site-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    }
   ],
   "source": [
    "output = draft_posts(\"AI engineer vs data scientist\",\n",
    "              gather_evidence(\"AI engineer vs data scientist\"), \n",
    "                load_voice_profile())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "72d6e87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are two LinkedIn post drafts comparing \"AI engineer vs data scientist\":\n",
      "\n",
      "---\n",
      "\n",
      "**- Variant A:**\n",
      "\n",
      "*   **Hook:** Confused about the difference between an #AIEngineer and a #DataScientist? You're not alone! While both roles are crucial in shaping AI projects, their core focus areas are quite distinct.\n",
      "*   **Body:** From my discussions and recent reads, it's clear: AI Engineers primarily focus on *building and deploying* AI-powered applications, often integrating closely with software development. Data Scientists, on the other hand, dive deep into *data analysis and predictive modeling*, extracting insights and informing decisions. Think of it as the builder versus the analyst – both essential for a successful AI ecosystem. Understanding your data is crucial, but so is bringing those models to life!\n",
      "*   **CTA:** Which path resonates more with your skills and career aspirations? Share your thoughts or experiences below!\n",
      "*   **Hashtags:** #AI #DataScience #MachineLearning #CareerGrowth #TechJobs #AIvsDS\n",
      "\n",
      "---\n",
      "\n",
      "**- Variant B:**\n",
      "\n",
      "*   **Hook:** The tech landscape is buzzing with roles like #AIEngineer and #DataScientist. While they often collaborate, understanding their unique contributions is key for anyone navigating a career in AI.\n",
      "*   **Body:** While there are definite overlaps, the distinctions are clear: AI Engineers lean heavily into software engineering to *operationalize* AI models and build robust applications. Data Scientists excel at *uncovering insights* from complex datasets, building predictive models, and driving data-driven strategy. Each role offers a unique career path with distinct skill sets – from deep learning deployment to advanced statistical analysis. Continuous learning is key in tech, and knowing where you fit helps immensely!\n",
      "*   **CTA:** What are your observations on how these roles collaborate effectively in real-world projects? Let's discuss!\n",
      "*   **Hashtags:** #AI #DataScience #CareerDevelopment #TechCareers #MachineLearning #Innovation #KnowledgeSharing\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8252586d",
   "metadata": {},
   "source": [
    "#### Compliance\n",
    "app/agents/compliance.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93b1f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from project_files.tools.moderation import check_toxicity\n",
    "\n",
    "def compliance_check(post_text):\n",
    "    issues = []\n",
    "    if len(post_text) > 1300:\n",
    "        issues.append(\"Exceeds LinkedIn char limit\")\n",
    "    if check_toxicity(post_text):\n",
    "        issues.append(\"Potentially toxic language detected\")\n",
    "    return issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fad408ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here are two LinkedIn post drafts comparing \"AI engineer vs data scientist\":\\n\\n---\\n\\n**- ',\n",
       " \" A:**\\n\\n*   **Hook:** Confused about the difference between an #AIEngineer and a #DataScientist? You're not alone! While both roles are crucial in shaping AI projects, their core focus areas are quite distinct.\\n*   **Body:** From my discussions and recent reads, it's clear: AI Engineers primarily focus on *building and deploying* AI-powered applications, often integrating closely with software development. Data Scientists, on the other hand, dive deep into *data analysis and predictive modeling*, extracting insights and informing decisions. Think of it as the builder versus the analyst – both essential for a successful AI ecosystem. Understanding your data is crucial, but so is bringing those models to life!\\n*   **CTA:** Which path resonates more with your skills and career aspirations? Share your thoughts or experiences below!\\n*   **Hashtags:** #AI #DataScience #MachineLearning #CareerGrowth #TechJobs #AIvsDS\\n\\n---\\n\\n**- \",\n",
       " \" B:**\\n\\n*   **Hook:** The tech landscape is buzzing with roles like #AIEngineer and #DataScientist. While they often collaborate, understanding their unique contributions is key for anyone navigating a career in AI.\\n*   **Body:** While there are definite overlaps, the distinctions are clear: AI Engineers lean heavily into software engineering to *operationalize* AI models and build robust applications. Data Scientists excel at *uncovering insights* from complex datasets, building predictive models, and driving data-driven strategy. Each role offers a unique career path with distinct skill sets – from deep learning deployment to advanced statistical analysis. Continuous learning is key in tech, and knowing where you fit helps immensely!\\n*   **CTA:** What are your observations on how these roles collaborate effectively in real-world projects? Let's discuss!\\n*   **Hashtags:** #AI #DataScience #CareerDevelopment #TechCareers #MachineLearning #Innovation #KnowledgeSharing\"]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.split(\"Variant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8dd3e2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A:**\\n\\n*   **Hook:** Confused about the difference between an #AIEngineer and a #DataScientist? You're not alone! While both roles are crucial in shaping AI projects, their core focus areas are quite distinct.\\n*   **Body:** From my discussions and recent reads, it's clear: AI Engineers primarily focus on *building and deploying* AI-powered applications, often integrating closely with software development. Data Scientists, on the other hand, dive deep into *data analysis and predictive modeling*, extracting insights and informing decisions. Think of it as the builder versus the analyst – both essential for a successful AI ecosystem. Understanding your data is crucial, but so is bringing those models to life!\\n*   **CTA:** Which path resonates more with your skills and career aspirations? Share your thoughts or experiences below!\\n*   **Hashtags:** #AI #DataScience #MachineLearning #CareerGrowth #TechJobs #AIvsDS\\n\\n---\\n\\n**-\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.split(\"Variant\")[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e270e584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compliance_check(output.split(\"Variant\")[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296ff916",
   "metadata": {},
   "source": [
    "#### Editor\n",
    "app/agents/editor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "523f3c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from project_files.configs import GEMINI_API_KEY, MODEL_NAME\n",
    "\n",
    "def refine_post(post_text):\n",
    "    llm = ChatGoogleGenerativeAI(model=MODEL_NAME, google_api_key=GEMINI_API_KEY)\n",
    "    prompt = f\"Edit the following LinkedIn post for clarity, conciseness, and impact:\\n{post_text}\"\n",
    "    return llm.invoke(prompt).content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b5bd10",
   "metadata": {},
   "source": [
    "#### LangGraph Orchestration\n",
    "app/graph.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7e817629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from project_files.agents.planner import create_brief\n",
    "# from project_files.agents.researcher import gather_evidence\n",
    "# from project_files.agents.writer import draft_posts\n",
    "# from project_files.agents.editor import refine_post\n",
    "# from project_files.agents.compliance import compliance_check\n",
    "# from project_files.tools.voice_profile import load_voice_profile\n",
    "\n",
    "def generate_linkedin_posts(topic):\n",
    "    voice_profile = load_voice_profile()\n",
    "    brief = create_brief(topic)\n",
    "    evidence = gather_evidence(topic)\n",
    "    drafts = draft_posts(topic, evidence, voice_profile)\n",
    "    \n",
    "    final_variants = []\n",
    "    for variant in drafts.split(\"Variant\"):\n",
    "        if not variant.strip():\n",
    "            continue\n",
    "        issues = compliance_check(variant)\n",
    "        if issues:\n",
    "            variant += f\"\\n\\n⚠ Issues: {issues}\"\n",
    "        refined = refine_post(variant)\n",
    "        final_variants.append(refined)\n",
    "    \n",
    "    return {\n",
    "        \"brief\": brief,\n",
    "        \"evidence\": evidence,\n",
    "        \"variants\": final_variants\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50a4fab",
   "metadata": {},
   "source": [
    "#### Model - Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "91a30047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amber/Documents/Learnings_2024/Linkdein_content_generator/linkdein_genai_env/lib/python3.10/site-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    }
   ],
   "source": [
    "result = generate_linkedin_posts(\"AI engineer vs data scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d90e26a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content Brief:\n",
      "Here's a LinkedIn content brief for the topic \"AI Engineer vs. Data Scientist\":\n",
      "\n",
      "---\n",
      "\n",
      "## LinkedIn Content Brief: AI Engineer vs. Data Scientist\n",
      "\n",
      "**Topic:** AI Engineer vs. Data Scientist: Understanding the Nuances for Career & Team Strategy\n",
      "\n",
      "**Audience:** Data professionals (Data Scientists, ML Engineers, Analysts, Data Leaders, Recruiters in the data space). They are familiar with the concepts but seek clarity on specialization, career progression, and team structure.\n",
      "\n",
      "---\n",
      "\n",
      "**Angle:**\n",
      "It's not about which role is \"better,\" but about understanding their distinct focuses, skillsets, and where they fit within the AI/ML lifecycle. This post will demystify the common confusion, helping individuals plot their career paths and organizations build more effective data teams. We'll highlight how these roles are complementary, not competitive, and essential for taking AI from concept to production.\n",
      "\n",
      "---\n",
      "\n",
      "**3 Key Points:**\n",
      "\n",
      "1.  **Beyond the Hype: Core Responsibilities & Impact:**\n",
      "    *   **Data Scientist:** Focused on *discovery, insights, and model prototyping*. Their strength lies in statistical analysis, exploratory data analysis (EDA), hypothesis testing, feature engineering, and building predictive models to answer business questions. They often work on the \"what\" and \"why.\"\n",
      "    *   **AI Engineer (or ML Engineer):** Focused on *productionization, deployment, and scalability*. Their strength is taking validated models from the data science team and integrating them into robust, efficient, and scalable production systems. They handle MLOps, infrastructure, performance optimization, and often work on the \"how.\"\n",
      "\n",
      "2.  **Divergent Skillsets & Tooling:**\n",
      "    *   **Data Scientist:** Strong in statistics, mathematics, domain expertise, data visualization, SQL, Python/R for analysis (e.g., Pandas, Scikit-learn, Matplotlib), and communication. Often works with Jupyter notebooks and BI tools.\n",
      "    *   **AI Engineer:** Strong in software engineering principles (clean code, testing, version control), MLOps (CI/CD, monitoring), cloud platforms (AWS, Azure, GCP), distributed systems, specific ML frameworks (TensorFlow, PyTorch), Docker, Kubernetes. Often works with IDEs and production environments.\n",
      "\n",
      "3.  **Career Trajectories & The Evolving Landscape:**\n",
      "    *   These roles are increasingly specialized, yet highly interdependent. A successful AI product requires both.\n",
      "    *   **Career Paths:** A Data Scientist might transition to an AI Engineer role if they develop strong software engineering skills, or become a Lead Data Scientist. An AI Engineer might specialize further into MLOps, ML Architect, or even lead an ML engineering team.\n",
      "    *   **The \"Full-Stack\" Dream:** While some aspire to be \"full-stack\" (DS + AIE), the depth required in both areas often makes it challenging. Organizations benefit more from clear role definitions and strong collaboration.\n",
      "\n",
      "---\n",
      "\n",
      "**Suggested Title:**\n",
      "\n",
      "*   **Option 1 (Question-based):** AI Engineer vs. Data Scientist: Are You Building Models or Building Systems?\n",
      "*   **Option 2 (Benefit-driven):** Demystifying AI Engineering vs. Data Science: Your Guide to Roles & Career Paths\n",
      "*   **Option 3 (Direct Comparison):** The Essential Difference: AI Engineer vs. Data Scientist in the Real World\n",
      "\n",
      "---\n",
      "\n",
      "**Desired Outcome:**\n",
      "\n",
      "*   **For Individuals:** Gain clarity on the distinct roles, helping them assess their skills, interests, and potential career paths within the AI/ML domain.\n",
      "*   **For Organizations/Leaders:** Better understand how to structure their data teams, define roles, and foster effective collaboration between Data Scientists and AI Engineers for successful AI product deployment.\n",
      "*   **Engagement:** Spark a lively discussion in the comments about individual experiences, career transitions, and the future of these roles.\n",
      "*   **Positioning:** Establish the content creator/company as a knowledgeable thought leader in the data and AI space.\n"
     ]
    }
   ],
   "source": [
    "print(\"Content Brief:\")\n",
    "print(result[\"brief\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2ee4b793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evidence:\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvidence:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5b60acf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Draft Variants:\n",
      "\n",
      "Variant 1:\n",
      "Okay, since the original drafts were not provided, I will create two *new* drafts based on the common distinctions between AI Engineer and Data Scientist roles, aiming for clarity, conciseness, and impact for a tech audience.\n",
      "\n",
      "Here are two options, choose the one that best fits your immediate goal or combine elements:\n",
      "\n",
      "---\n",
      "\n",
      "### **Option 1: Focus on \"Which Path is Right For You?\"**\n",
      "\n",
      "Navigating the tech landscape, it's easy to confuse the **AI Engineer** and **Data Scientist** roles. While both work with data and AI, their primary focuses are distinct yet complementary. Let's break it down:\n",
      "\n",
      "**📊 Data Scientist:**\n",
      "The master of insights. Data Scientists focus on **exploratory data analysis, statistical modeling, and predictive analytics** to uncover patterns and answer complex business questions. They're about the \"why\" and \"what if,\" using data to drive strategic decisions.\n",
      "*   **Key skills:** Statistics, machine learning algorithms, SQL, data visualization, strong communication.\n",
      "\n",
      "**🤖 AI Engineer:**\n",
      "The builder and deployer. AI Engineers take models from concept to reality, focusing on **designing, building, and deploying production-ready AI/ML systems**. They ensure models are scalable, efficient, and integrated into larger software ecosystems. They're about the \"how\" and \"making it work reliably.\"\n",
      "*   **Key skills:** Software engineering, MLOps, deep learning frameworks, cloud platforms, containerization (Docker, Kubernetes).\n",
      "\n",
      "**The Synergy:** They're two sides of the same coin, often collaborating closely. A Data Scientist might develop a groundbreaking model, and an AI Engineer then scales and deploys it for real-world use.\n",
      "\n",
      "**Which path resonates with you?** If your passion lies in uncovering the 'why' behind data and driving strategic decisions, Data Science might be your calling. If you thrive on building, deploying, and scaling robust AI applications, the AI Engineering path awaits.\n",
      "\n",
      "#AI #DataScience #AIEngineer #MachineLearning #CareerPath #TechJobs #MLOps #BigData\n",
      "\n",
      "---\n",
      "\n",
      "### **Option 2: Focus on \"A Deep Dive into Role Distinctions\"**\n",
      "\n",
      "**AI Engineer vs. Data Scientist: What's the real difference?** 🤔\n",
      "\n",
      "These two critical roles often get conflated, but they serve distinct purposes in the AI/ML lifecycle. Understanding their core responsibilities is key for aspiring professionals and hiring managers alike.\n",
      "\n",
      "**🎯 Data Scientist:**\n",
      "*   **Primary Goal:** To extract insights from complex datasets, build predictive models, and inform business strategy.\n",
      "*   **Core Responsibilities:** Exploratory Data Analysis (EDA), statistical modeling, hypothesis testing, predictive analytics, data visualization, communicating findings to stakeholders.\n",
      "*   **Focus:** Problem identification, data understanding, model development.\n",
      "\n",
      "**⚙️ AI Engineer:**\n",
      "*   **Primary Goal:** To design, build, and deploy production-ready AI/ML systems and infrastructure.\n",
      "*   **Core Responsibilities:** Model deployment, MLOps, pipeline optimization, system integration, performance monitoring, deep learning application development, ensuring scalability and reliability.\n",
      "*   **Focus:** System architecture, model operationalization, performance, and maintenance.\n",
      "\n",
      "While both require a solid understanding of machine learning principles, Data Scientists lean into statistical rigor and business acumen, while AI Engineers emphasize software engineering best practices and system reliability.\n",
      "\n",
      "They are complementary roles, with Data Scientists often innovating on models and AI Engineers bringing those innovations to life at scale.\n",
      "\n",
      "What are your observations on these roles? Share your insights below! 👇\n",
      "\n",
      "#AI #DataScience #AIEngineer #MachineLearning #TechCareers #MLOps #SoftwareEngineering #DataAnalytics\n",
      "\n",
      "---\n",
      "\n",
      "**Key Changes Made and Why:**\n",
      "\n",
      "*   **Clearer Hooks:** Started with direct questions or statements that immediately set up the comparison.\n",
      "*   **Concise Definitions:** Used bolding and bullet points to make the core responsibilities and skills scannable and easy to digest.\n",
      "*   **Action Verbs:** Used strong verbs (e.g., \"uncover,\" \"drive,\" \"design,\" \"deploy,\" \"ensure\") to describe roles.\n",
      "*   **\"Why\" vs. \"How\" Analogy:** This is a common and effective way to quickly differentiate the two roles.\n",
      "*   **Emphasis on Synergy:** Highlighted that they are complementary roles, not competitive, which is important for a collaborative tech environment.\n",
      "*   **Direct Call to Action (CTA):** Encouraged engagement with specific questions relevant to the post's content.\n",
      "*   **Relevant Hashtags:** Broadened the reach and categorized the content effectively.\n",
      "*   **Emojis:** Used sparingly to add visual appeal and break up text, without being unprofessional.\n",
      "*   **Removed Redundancy:** Streamlined sentences and paragraphs to avoid repeating information.\n",
      "*   **Audience Focus:** Maintained a professional yet engaging tone suitable for LinkedIn's tech audience.\n",
      "\n",
      "\n",
      "Variant 2:\n",
      "Here's an edited version focusing on clarity, conciseness, and impact for LinkedIn:\n",
      "\n",
      "---\n",
      "\n",
      "**Option 1 (Concise & Direct):**\n",
      "\n",
      "**Hook:** Demystifying AI Careers: The core difference between an AI Engineer and a Data Scientist is a common point of confusion. Understanding it is key for anyone navigating the tech landscape.\n",
      "\n",
      "**Body:** While both roles are vital for developing AI systems, their core focuses diverge significantly:\n",
      "\n",
      "*   **Data Scientists:** Interpret data, focusing on the \"what\" and \"why\" to extract actionable insights. They discover the patterns.\n",
      "*   **AI Engineers:** Build the systems and infrastructure to implement those insights, focusing on the \"how\". They're the architects and builders, turning insights into functional AI.\n",
      "\n",
      "**CTA:** Which role aligns more with your skills and aspirations? Share your insights or experiences in the comments!\n",
      "\n",
      "**Hashtags:** #AI #DataScience #AIEngineer #DataScientist #MachineLearning #TechCareers #CareerGrowth\n",
      "\n",
      "---\n",
      "\n",
      "**Option 2 (Slightly more descriptive, still concise):**\n",
      "\n",
      "**Hook:** Ever wondered about the distinct roles of an AI Engineer vs. a Data Scientist on AI project teams? It's a crucial distinction for career growth in tech!\n",
      "\n",
      "**Body:** Both roles are indispensable for developing AI systems and applications, yet their primary focus areas are quite different:\n",
      "\n",
      "*   **Data Scientists:** Specialize in interpreting data, answering the \"what\" and \"why\" to draw actionable conclusions and extract key insights. They are the discoverers.\n",
      "*   **AI Engineers:** Focus on building the robust systems that enable AI functionalities. They are the architects and construction workers, translating insights into scalable, autonomous \"how-to\" solutions.\n",
      "\n",
      "**CTA:** Which role resonates more with your expertise and career aspirations? Share your thoughts or experiences below!\n",
      "\n",
      "**Hashtags:** #AI #DataScience #AIEngineer #DataScientist #MachineLearning #TechCareers #CareerPath #AIJobs\n",
      "\n",
      "---\n",
      "\n",
      "**Key Changes Made and Why:**\n",
      "\n",
      "*   **Hook:** Made more direct and impactful. Removed \"I hear often\" as it's less professional for LinkedIn. Clearly stated the value proposition upfront.\n",
      "*   **Body:**\n",
      "    *   Used bullet points for better readability and to clearly separate the roles.\n",
      "    *   Tightened language (\"extracting insights!\" became \"extract actionable insights\").\n",
      "    *   Refined the \"architects and construction workers\" analogy to be more integrated with the \"how\" aspect.\n",
      "    *   Removed redundant sentences like \"Both are vital, but their day-to-day looks quite different\" as it was already implied.\n",
      "*   **CTA:** Made more concise. Removed the generic \"Continuous learning is key...\" as the post itself promotes learning by providing the distinction. Kept the call to action clear.\n",
      "*   **Hashtags:** Kept relevant, ensuring they're specific to the roles mentioned while also including broader terms like #TechCareers and #CareerGrowth.\n",
      "\n",
      "\n",
      "Variant 3:\n",
      "Here's an edited version of your LinkedIn post, focusing on clarity, conciseness, and impact:\n",
      "\n",
      "---\n",
      "\n",
      "**AI Engineer vs. Data Scientist: Demystifying Key Roles**\n",
      "\n",
      "Navigating AI careers? Understanding the nuances between an AI Engineer and a Data Scientist is crucial. While distinct, these roles are deeply intertwined in bringing AI solutions to life.\n",
      "\n",
      "**Data Scientists** excel at uncovering the \"what\" and \"why\" – finding hidden stories and deriving actionable insights from complex datasets. They're the master interpreters.\n",
      "\n",
      "**AI Engineers** then build the \"how,\" transforming these insights into robust, automated, and scalable AI applications that operate without constant human oversight. They're the architects of deployment.\n",
      "\n",
      "In essence: Data Scientists discover the intelligence; AI Engineers build the systems that *use* that intelligence.\n",
      "\n",
      "What's your take? Have you worked in either role, or are you considering a switch? Share your insights below and help others navigate their AI career path!\n",
      "\n",
      "#AI #MachineLearning #DataScience #AIEngineering #CareerPath #TechCareers #MLOps\n",
      "\n",
      "---\n",
      "\n",
      "**Here's a breakdown of the changes and why they were made:**\n",
      "\n",
      "*   **Hook:**\n",
      "    *   Original: \"Navigating the world of AI roles can be complex, especially when choosing between becoming an AI Engineer or a Data Scientist. Let's demystify these powerful positions!\"\n",
      "    *   Edited: \"AI Engineer vs. Data Scientist: Demystifying Key Roles\" (Strong, direct headline). Followed by \"Navigating AI careers? Understanding the nuances between an AI Engineer and a Data Scientist is crucial.\" (More concise and engaging intro).\n",
      "*   **Body:**\n",
      "    *   Removed \"It's fascinating how...\" – Unnecessary filler.\n",
      "    *   Streamlined the descriptions of each role for conciseness.\n",
      "    *   Used bolding for \"Data Scientists\" and \"AI Engineers\" to make the distinction clearer and more scannable.\n",
      "    *   Combined the \"what/why\" and \"how\" into more direct statements for each role.\n",
      "    *   Added a concluding summary sentence: \"In essence: Data Scientists discover the intelligence; AI Engineers build the systems that *use* that intelligence.\" This provides a powerful, memorable distinction.\n",
      "    *   Removed \"Understanding your data is crucial, but so is building scalable solutions!\" as it was somewhat redundant given the detailed descriptions.\n",
      "*   **CTA:**\n",
      "    *   Removed the generic \"Networking is not about collecting contacts, it's about building relationships!\" as it was off-topic for the specific content of the post and diluted the call to action.\n",
      "    *   Made the questions more direct and action-oriented.\n",
      "    *   Changed \"make informed decisions\" to \"navigate their AI career path\" for a more specific and helpful tone.\n",
      "*   **Hashtags:**\n",
      "    *   Added #DataScience and #AIEngineering for direct relevance.\n",
      "    *   Changed #CareerAdvice to #CareerPath for a slightly more active feel.\n",
      "    *   Kept the most relevant and impactful tags.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDraft Variants:\")\n",
    "for i, variant in enumerate(result[\"variants\"], 1):\n",
    "    print(f\"\\nVariant {i}:\\n{variant}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a78a37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5409e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linkdein_genai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
